defaults:
  - _self_

task: selfies

compute:
  ngpus: 1  # Single GPU for debugging
  nodes: 4

logging:
  log_freq: 10  # More frequent logging for debugging
  log_lr_every: ${logging.log_freq}
  log_file_name: stdout.log
  enable_wandb: False  # Disable wandb for test run
  entity: maximilianholsman
  project: Gated proposal model
  group: null

data:
  train: fasta  # Use FASTA dataset
  valid: fasta  # Use same dataset for validation
  fasta_path: /scratch/pranamlab/tong/cope/editflows/data/cas9_dataset_fasta.fa
  # fasta_path: /usr/xtmp/mth45/Documents/programmable_biology_group/EditFlows/flow_matching/examples/editflows/data/example_data.fa
  cache_dir: null  # Not needed for FASTA
  num_workers: 2  # Reduced for debugging

training:
  batch_size: 4  # Small batch size for debugging
  snapshot: 100  # Frequent checkpoints
  eval_freq: 1000  # Frequent evaluation
  perplexity_freq: 100  # Less frequent generation
  seed: 42

eval:
  batch_size: 4
  sample_batch_size: 4
  perplexity: False  # Disable perplexity for test run
  perplexity_batch_size: 4
  lm_name: 

optim:
  weight_decay: 0.03
  optimizer: AdamW
  lr: 5e-4
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  grad_clip: 1.
  eta_min_ratio: 0.1
  fused: false
  n_epochs: 100
  warmup_ratio: 0.1

flow:
  source_distribution: uniform  # [uniform, mask]
  loss_function: editflows  # [editflows, cross_entropy, generalized_kl]
  exponent: 2.
  scheduler_type: polynomial
  sampling_steps: 256  # Fewer sampling steps for debugging
  eps_id: -1  # Epsilon token ID (must NOT be a valid vocab id)

model:
  hidden_size: 768  # Smaller model for debugging
  cond_dim: 128  # 1/4 of hidden_size is a reasonable starting point
  length: 2048  # Shorter sequence length
  n_blocks: 8  # Fewer layers
  n_heads: 12
  dropout: 0.1
  compile: False  # Disable compile for easier debugging
  esm_model_name: facebook/esm2_t33_650M_UR50D
  freeze_esm: true
  p_optimal: 0
  scale_size: 1.5

work_dir: /scratch/pranamlab/tong/cope/editflows/outputs
